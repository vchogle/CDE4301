<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>AI simulated patient for dentistry student communication training in VR</title>
  <link rel="icon" href="./favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/themes/light.css" />
  <script type="module"
    src="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/shoelace-autoloader.js"></script>

  <link rel="stylesheet" href="./index.css">

  <link rel="stylesheet" href="./components/team-member/team-member.css">
  <script type="module" src="./components/team-member/team-member.js"></script>

  <link rel="stylesheet" href="./components/table-of-content/table-of-content.css">

  <script type="module" src="./components/image/image-component.js"></script>

  <script type="module" src="./components/video/video.js"></script>

  <link rel="stylesheet" href="./components/references/references.css">

  <link rel="stylesheet" href="./components/scroll-to-top/scroll-to-top.css">
  <script src="./components/scroll-to-top/scroll-to-top.js"></script>

  <script src="./components/table-component/table-component.js"></script>

  <link href="https://unpkg.com/gridjs/dist/theme/mermaid.min.css" rel="stylesheet" />
</head>

<body>
  <div class="content">
    <h1>VR‚Äê406: AI simulated patient for dentistry student communication training in VR</h1>
    <h4>By: Chogle Vidita Nikhil - Year 4 Biomedical Engineering</h4>

    <!-- This is the team member component use to display details about your team members -->

    <!-- This is a divide from the shoelace library for aesthetic purpose -->
    <sl-divider></sl-divider>

    <!-- This is the table-of-content component use to define all of the link directly to each section -->
    <div class="table-of-content">
      <h2>Table of Contents</h2>
      <sl-tree>
        <sl-tree-item><a href="#acknowledgements">Acknowledgements</a></sl-tree-item>
    
        <sl-tree-item><a href="#section-header-1">1. Introduction</a></sl-tree-item>
    
        <sl-tree-item>
          <a href="#section-header-2">2. Problem Definition and Context</a>
          <sl-tree-item><a href="#sub-section-2-header-1">2.1 Problem Definition</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-2-header-2">2.2 Value Proposition</a></sl-tree-item>
        </sl-tree-item>
    
        <sl-tree-item>
          <a href="#section-header-3">3. Literature Review and Context</a>
          <sl-tree-item><a href="#sub-section-3-header-1">3.1 Literature Review</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-3-header-2">3.2 Past Project</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-3-header-3">3.3 Key Insights Behind Concept Development</a></sl-tree-item>
        </sl-tree-item>
    
        <sl-tree-item>
          <a href="#section-header-4">4. Concept Development</a>
          <sl-tree-item><a href="#sub-section-4-header-1">4.1 Design Rationale and System Goals</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-4-header-2">4.2 Exploration of Concept Alternatives</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-4-header-3">4.3 Justification for Final Concept Direction</a></sl-tree-item>
        </sl-tree-item>
    
        <sl-tree-item>
          <a href="#section-header-5">5. Design Specifications</a>
          <sl-tree-item><a href="#sub-section-5-header-1">5.1 Functional Requirements</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-5-header-2">5.2 Non-Functional Requirements</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-5-header-3">5.3 Technical Constraints and Assumptions</a></sl-tree-item>
        </sl-tree-item>
    
        <sl-tree-item>
          <a href="#section-header-6">6. System Architecture</a>
          <sl-tree-item><a href="#sub-section-6-header-1">6.1 Backend (Python)</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-6-header-2">6.2 Frontend (Unity 3D)</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-6-header-3">6.3 Communication Workflow</a></sl-tree-item>
        </sl-tree-item>
    
        <sl-tree-item>
          <a href="#section-header-7">7. Prototyping and Implementation</a>
          <sl-tree-item><a href="#sub-section-7-header-1">7.1 Prototype 1: Knowledge Chatbot</a>
            <sl-tree-item><a href="#sub-section-7-header-1-1">7.1.1 Prompt Engineering Process</a></sl-tree-item>
            <sl-tree-item><a href="#sub-section-7-header-1-2">7.1.2 Technical Setup</a></sl-tree-item>
          </sl-tree-item>
          <sl-tree-item><a href="#sub-section-7-header-2">7.2 Prototype 2: Immersive VR Patient</a>
            <sl-tree-item><a href="#sub-section-7-header-2-1">7.2.1 System Architecture</a></sl-tree-item>
            <sl-tree-item><a href="#sub-section-7-header-2-2">7.2.2 Backend (Python)</a></sl-tree-item>
            <sl-tree-item><a href="#sub-section-7-header-2-3">7.2.3 Frontend (Unity)</a></sl-tree-item>
            <sl-tree-item><a href="#sub-section-7-header-2-4">7.2.4 Scene Design and VR Interaction</a></sl-tree-item>
            <sl-tree-item><a href="#sub-section-7-header-2-5">7.2.5 Speech API Integration and Unity Timeline</a></sl-tree-item>
          </sl-tree-item>
        </sl-tree-item>
    
        <sl-tree-item>
          <a href="#section-header-8">8. Testing and Evaluation</a>
          <sl-tree-item><a href="#sub-section-8-header-1">8.1 Preliminary Testing with Non-Dental Students</a></sl-tree-item>
          <sl-tree-item><a href="#sub-section-8-header-2">8.2 Domain-Specific Testing with Dental Students</a></sl-tree-item>
        </sl-tree-item>
    
        <sl-tree-item><a href="#section-header-9">9. Fulfilment of Deliverables</a></sl-tree-item>
        <sl-tree-item><a href="#section-header-10">10. Reflections and Future Work</a></sl-tree-item>
        <sl-tree-item><a href="#section-header-11">11. Conclusion</a></sl-tree-item>
        <sl-tree-item><a href="#references">References</a></sl-tree-item>
        <sl-tree-item><a href="#appendix">Appendix</a></sl-tree-item>
      </sl-tree>
    </div>    
    <sl-divider></sl-divider>

    <div>

      <!-- This is an example of what a section might look like -->
      <div id="acknowledgements">
        <h2>Acknowledgements</h2>
        <p>I would like to sincerely thank my CDE4301 project supervisor, Professor Khoo Eng Tat, for his guidance and support throughout the development of this project. I am also grateful to the NUS Faculty of Dentistry, especially Professor Wong Mun Loke, for his insights and kind cooperation during the course of this work. I would like to thank Aiden Koh from the Immersive Reality Lab for his technical support and for helping me get started with the tools and systems used in this project.</p>
        <p>I would also like to thank Harshita and Jiali from the CDE3301 project for their support this semester, especially during the prototype demonstration. Lastly, I am thankful to the dental students who took the time to try out the prototype and share their feedback. It played an important role in shaping the direction of the final version.</p>

      <div id="section-header-1">
        <h2> 1. Introduction </h2>
          <p>By 2030, 1 in 4 Singaporeans will be aged 65 or older, leading to an increase in geriatric patients requiring effective and timely medical care, particularly in dentistry (ACTION PLAN FOR SUCCESSFUL AGEING 2023, 2024). In geriatric dentistry, age-related factors and behaviors, such as care rejection, aggression, and agitation, can complicate treatment. Poor communication with elderly patients often results in diminished care quality and unsuccessful dental outcomes. The main issue addressed in this project is the limited exposure and communication training dentistry students receive when interacting with geriatric patients.</p>
          <p>This project aims to develop an innovative solution integrating artificial intelligence (AI) and virtual reality (VR) to create an AI-simulated patient. This solution will enhance the communication training of dentistry students by immersing them in a realistic dental office setting where they can interact with a 3D virtual patient. This report outlines the background and context of the problem, a review of related work, the development of the proposed concept, detailed design specifications, prototyping and implementation progress, system testing and evaluation, and a summary of how the key project deliverables have been fulfilled. It concludes with reflections on the project and recommendations for future work.</p>
          <p>In 2023, a previous project involved students utilizing a platform with a virtual patient presented in a 2D format. However, this approach faced several challenges. The virtual patient platform is no longer in use, prompting the need for alternative solutions. Additionally, the 2D animated patient lacked the immersive experience required to effectively engage dentistry students. The large language model used in the project also struggled with providing accurate responses, underscoring the need for enhanced training data to improve its performance.</p>
          <p>This project addresses these limitations by introducing a virtual reality (VR) component, enabling students to interact with a 3D virtual patient within a realistic dental office environment. This immersive setting enables direct, spoken interaction between the dental student and a simulated geriatric patient, allowing the student to practise communication strategies essential for accurate diagnosis and treatment planning.</p>
    </div>

    <br />
    <sl-divider></sl-divider>
    <div>
      <div id="section-header-2">
        <h2>2. Problem Definition and Context</h2>
        <div id="sub-section-2-header-1">
          <h3>2.1 Problem Definition</h3>
          <p>The NUS Faculty of Dentistry currently employs SPs as part of its curriculum to improve students' communication skills. A standardized patient is an actor who has been meticulously trained to portray a real patient so convincingly that even a skilled clinician cannot distinguish the simulation from an actual patient encounter (Hillier et al., 2024). This hands-on approach allows dentistry students to practice their interpersonal and diagnostic skills in a controlled, simulated environment, preparing them for real-world scenarios. </p>

          <p>However, while effective, this method has significant drawbacks. SPs are resource-intensive, requiring actors to be rigorously trained to simulate a wide range of conditions accurately. Coordinating SP sessions involves scheduling complexities, logistical constraints, and significant financial costs. Moreover, SP interactions are limited to specific scenarios, which can restrict the variety of patient behaviors and cases that students can experience. These limitations highlight the need for an alternative, scalable solution that maintains the benefits of SP-based training while addressing its resource-heavy nature. </p> 
          
          <p>This project focuses on improving communication training for dentistry students, with an emphasis on interactions with geriatric patients. Effective communication is critical in ensuring accurate diagnoses and successful treatment outcomes, particularly for older patients who may have complex medical and behavioral needs. By introducing an AI-simulated patient in a virtual reality (VR) environment, the project seeks to offer an immersive and diverse training experience that goes beyond what SPs can provide.  </p>
          
          <p>In the broader context, this project also aims to support the healthcare sector's response to Singapore's aging population. By equipping future dentists with advanced communication skills and experience interacting with geriatric patients, the initiative contributes to better dental care outcomes and improved quality of life for elderly individuals. Additionally, the adoption of AI and VR technology in educational training promotes innovation, potentially setting a precedent for other areas of healthcare education. Finally, reducing the reliance on SPs not only saves resources but also creates a scalable and adaptable training framework that can be expanded to cover a wider range of patient cases and scenarios in the future.</p>
          
      </div>
      <div id="sub-section-2-header-2">
        <h3>2.2 Value Proposition</h3>
        <p>To address the current state in dental education, this project introduces an AI-simulated patient in a virtual reality environment designed to provide dentistry students with immersive communication training focused on geriatric care.</p>
        <p>The platform enables students to engage in realistic, voice-based conversations with an elderly patient within a fully immersive dental clinic setting. This approach creates a safe and controlled environment where students can build confidence and refine their interpersonal skills, particularly when navigating the complexities of age-related behaviors and communication barriers.</p>
        <p>By offering consistent and repeatable training experiences, the solution reduces reliance on standardized patients, making communication training more scalable, cost-effective, and accessible across institutions.</p>
        <p>Through realistic interactions, this platform equips future dentists with the empathy, adaptability, and communication proficiency needed to deliver effective and compassionate care to an aging population.</p>
      </div>

      <br />
      <sl-divider></sl-divider>
      <div id="section-header-3">
        <h2> 3. Literature Review and Context </h2>
        <div id="sub-section-3-header-1">
          <h3>3.1 Literature Review</h3> 
          <p>To address the limitations of standardized patients, I began by conducting a literature review to explore current approaches for simulating these patient interactions using AI and large language models (also known as LLM). I examined several papers and selected one published before the introduction of LLMs and one published afterward.</p>
          <p>In the pre-LLM phase, a virtual patient named Julia was developed to address dental problems. An informal-language chatbot was designed to answer both clinical and non-clinical questions. In a descriptive cross-sectional study, 193 students engaged with the AI chatbot over several weeks to practice diagnosing clinical cases. However, this approach had its limitations. The chatbot technology did not allow dental students to directly interact with a patient, failing to simulate the appropriate clinical environment. Additionally, students expressed a preference for face-to-face interactions over using the chatbot, indicating a gap in the immersive experience (Su√°rez et al., 2022).</p>
          <p>In the post-LLM phase, the Furhat Robot was introduced as a more advanced virtual patient. It featured an animated face projected onto a translucent mask and supported multiple speech synthesis options, allowing it to adopt various identities, including different genders, ages, and ethnicities. The system was developed using the FurhatSDK and integrated with OpenAI's GPT-3.5-turbo model for conversational capabilities. Despite these advancements, limitations persisted. Users often experienced confusion when the robot heard their utterance but failed to respond immediately. Moreover, as a static robot, it lacked the ability to convey hand gestures or other non-verbal cues, which are essential components of human interaction (Borg et al., 2024).</p>
        </div>
        <div id="sub-section-3-header-2">
          <h3>3.2 Past Project</h3> 
          <p>To develop this solution, past methodologies were examined, including a project conducted last year that introduced desktop simulation software to help dental students improve communication with virtual geriatric patients. The solution consisted of various modules, such as InteractAI as the platform for the virtual patient, Microsoft Azure for real-time speech-to-text and text-to-speech capabilities powered by neural networks, OpenAI GPT-3.5 for response generation, and OpenAI GPT-4.0 for post-conversation analysis. However, the solution faced several limitations. The virtual patient platform, InteractAI, is no longer in use, necessitating the exploration of alternative platforms. Additionally, the virtual patient was presented in 2D, which lacked the immersive environment needed to engage dentistry students effectively. Furthermore, the large language model required more accurate responses, highlighting the need for additional data to improve its training and performance.</p>
        </div>
        <div id="sub-section-3-header-3">
          <h3>3.3 Key Insights Behind Concept Development</h3>
          <p>The literature and related work shaped the direction of this project significantly. Reviewing both pre- and post-LLM virtual patient systems, it became clear that earlier chatbot-based platforms helped lay the groundwork but lacked the realism and interactivity needed for clinical communication training. For instance, the Julia chatbot (Su√°rez et al., 2022) allowed students to engage with simple case prompts via text, but the absence of embodiment and spoken dialogue reduced authenticity and engagement.</p>
          <p>Newer systems like the Furhat Robot (Borg et al., 2024) introduced more advanced interaction using animated faces and GPT-3.5 integration. However, they still had limitations such as response delays, limited expressiveness, and a static physical setup. These issues showed that while LLMs had improved, they needed a more immersive and emotionally rich platform to be effective in education.</p> 
          <p>Separately, past project work with InteractAI, Azure speech tools, and 2D simulations highlighted the same problem. Students lost interest without a strong sense of presence. The flat 2D environment and lack of embodied behavior made it difficult to simulate emotional or cognitive conditions like dementia. Early prompt testing also revealed that LLMs require domain-specific tuning to stay consistent in character over time.</p>
          <p>Together, these findings pointed to the need for a system that combines immersive realism, expressive communication, and LLM adaptability. This directly shaped the decision to build a VR-based platform powered by GPT-4o, hosted in a 3D clinical setting with simulated patient personas. It became clear that effective communication training needs more than just conversation. Emotional tone and memory-driven behavior are essential for creating a realistic and educationally meaningful experience.</p>
        </div>
      </div>

    <br />
    <sl-divider></sl-divider>
    <div id="section-header-4">
      <h2> 4. Concept Development </h2>
      <div id="sub-section-4-header-1">
        <h3>4.1 Design Rationale and System Goals</h3> 
        <p>This project started from a clear gap in how dental students are trained to communicate with older patients, especially those showing signs of cognitive decline. While standardized patients offer useful exposure, they are often limited in availability, consistency, and cost. Similarly, most existing digital tools do not provide emotionally realistic interactions or a convincing clinical environment. The main goal of this system is to simulate a realistic, responsive geriatric patient who can engage students in meaningful conversations. This includes presenting challenges commonly seen in older patients, such as forgetfulness, anxiety, or confusion, and encouraging students to adapt their tone, phrasing, and empathy accordingly.</p>
        <p>My aim was to create a learning tool that combines conversational AI with virtual reality in a way that feels lifelike and educational. I wanted students to be able to speak naturally to a virtual patient, receive emotionally appropriate responses, and feel as though they were in an actual dental consultation. Rather than just building another chatbot, the system is designed to simulate a full dental visit with a believable elderly character who has their own personality, backstory, and cognitive challenges.</p>
      </div>
      <div id="sub-section-4-header-2">
        <h3>4.2 Exploration of Concept Alternatives</h3> 
        <p>During the early design phase, I explored multiple directions for the system. One of the first key decisions was choosing the platform: web, desktop, or VR. While web-based and desktop solutions were more accessible and technically straightforward, they lacked the level of immersion needed for realistic training. Previous projects using 2D virtual patients, such as those built on InteractAI, mostly involved typing and reading text, which did not reflect how actual dental consultations take place.</p>
        <p>After discussions with the Immersive Reality Lab, I decided to pursue a VR-based approach. VR provided a stronger sense of presence by situating students in a simulated clinic environment, making it easier to support authentic communication training.</p>
        <p>For the patient design, I experimented with a range of geriatric personas, including a cooperative elderly patient, one who appeared more anxious, and one exhibiting early signs of dementia. Testing these different archetypes helped clarify how each would require unique prompting strategies, emotional expressions, and memory behavior. I also compared scripted dialogue trees with open-ended conversations powered by large language models. Table 1 below shows a summary of this test. The natural flow and adaptability of LLMs made them a better fit for simulating complex, real-time communication scenarios. </p>
        <figure>
          <img src="./assets/table1.png" alt="Table 1" style="max-width: 100%; height: auto;" />
          <figcaption><i>Table 1: Summary of findings of test with ChatGPT</i></figcaption>
        </figure>
      </div>
      <div id="sub-section-4-header-3">
        <h3>4.3 Justification for Final Concept Direction</h3> 
        <p>After testing initial versions using the VR system developed for cabin crew training and reviewing feedback from previous projects, I committed to a VR-based concept powered by a large language model. Using GPT-4o mini allows the virtual patient to respond in a way that feels emotionally aware and naturally conversational. This was important for simulating geriatric patients who may struggle with memory, repeat themselves, or display shifts in mood. These are moments where students need to respond with empathy, and the system is designed to support that learning experience.</p>
        <p>VR was chosen to provide a fully immersive environment where students can practise not just verbal communication, but also situational awareness and emotional presence. While the focus of this iteration is on conversation, the 3D clinical setting adds depth and realism to the experience. It helps students feel like they are in a real dental consultation, with the opportunity to adjust their responses based on how the patient is behaving or reacting.</p>
        <p>In summary, this concept was selected because it strikes the right balance between realism, flexibility, and educational value. It supports scalable and repeatable training without losing the human complexity that is often missing from traditional tools. With a foundation built on GPT-4o, Unity 3D, and validated communication scripts, the system can be expanded in the future to include more patient profiles, feedback tools, or adaptive difficulty based on performance.</p>
      </div>
  </div> 

  <br />
  <sl-divider></sl-divider>
  <div id="section-header-5">
    <h2> 5. Design Specifications </h2>

    <div id="sub-section-5-header-1">
      <h3>5.1 Functional Requirements</h3>
      <p>The solution is designed to simulate realistic communication between dentistry students and elderly patients through the integration of AI and virtual reality. Students should be able to speak naturally to the virtual patient using a microphone, while the AI responds using spoken dialogue generated by a large language model and delivered via a text-to-speech engine. These interactions must take place within a realistic dental scenario, with the system maintaining context and expressing appropriate emotional tones. The VR environment should resemble an actual dental clinic to help students build familiarity with a professional treatment setting. The system should also be able to simulate a range of communication styles, such as cooperative, confused, or anxious patients, to reflect real-world diversity. After each session, students should receive structured feedback aligned with communication assessment rubrics, covering aspects like clarity, empathy, and the use of follow-up questions. The virtual patient should also exhibit non-verbal behaviors, such as facial expressions or hesitation, to further reinforce realistic communication.</p>
    </div>
    <div id="sub-section-5-header-2">
      <h3>5.2 Non-Functional Requirements</h3>
      <p>In addition to core features, the system must meet several non-functional expectations. Low latency is essential. AI responses should ideally occur within two seconds to maintain natural conversation flow. The platform should also be scalable, allowing future integration of new patient profiles and training scenarios. The AI must remain in character and follow a structured conversational flow, especially for sensitive personas like Shibing, who simulates dementia. Usability is another key consideration; students should be able to use the system without extensive onboarding, and the VR experience should be smooth and intuitive. Finally, reliability is critical. The system must run consistently during sessions without crashing or interrupting the training process.</p>
    </div>
    <div id="sub-section-5-header-3">
      <h3>5.3 Technical Constraints and Assumptions</h3>
      <p>The system is built using Unity 3D and deployed on MetaQuest VR headsets. AI responses are powered by the GPT-4o mini model, accessed via API, which requires a stable internet connection for reliable performance. The original codebase provided was optimized for Windows, so early development included resolving compatibility issues across platforms. The AI training data is based on validated dentist-patient communication scripts provided by the Faculty of Dentistry. This version of the prototype assumes that users have basic familiarity with conversational English and are comfortable using standard VR controls.</p>
    </div>
      </div>

      <br />
      <sl-divider></sl-divider>
      <div id="section-header-6">
        <h2> 6. System Architecture </h2>
        <p>The system architecture of the virtual patient training environment is organized into two primary components: a Python-based backend that manages AI-driven dialogue and behavioral logic, and a Unity-based frontend that presents the immersive VR experience. These components are integrated via a WebSocket communication interface, enabling seamless real-time conversations between the dental student and the AI-simulated patient.</p>
        <div id="sub-section-6-header-1">
          <h3>6.1 Backend (Python)</h3>
          <p>The backend is designed with modularity and extensibility in mind, enabling swift iteration of behavior scripts, patient personas, and interaction pipelines. It includes the following core modules:</p>
          <p>This backend is lightweight and portable, supporting deployment both on a local workstation and cloud servers. Patient behavior and speech style can be customized by modifying prompt templates and fallback logic, making the system easily adaptable for simulating other patient archetypes (e.g., anxious, stoic, uncooperative).</p>
        </div>
        <div id="sub-section-6-header-2">
          <h3>6.2 Frontend (Unity 3D)</h3>
          <p>The Unity frontend renders a high-fidelity virtual dental clinic where students interact with the patient using a VR headset. It comprises the following subsystems:</p>
        </div>
        <div id="sub-section-6-header-3">
          <h3>6.3 Communication Workflow</h3>
          <p>The complete interaction pipeline follows this sequence:</p>
          <ol>
            <li>The student speaks to the virtual patient through the VR headset.</li>
            <li>Unity records the audio, sends it to Azure STT, and transmits the transcript to the Python backend.</li>
            <li>The backend formats the input with patient-specific behaviors and sends a prompt to GPT-4o.</li>
            <li>GPT-4o generates a response, which is returned as text.</li>
            <li>The backend converts the response into audio using Azure TTS and sends it to Unity.</li>
            <li>Unity plays the audio and animates the patient‚Äôs face based on embedded emotional cues.</li>
          </ol>
        </div>
      </div> 

      <br />
      <sl-divider></sl-divider>
      <div id="section-header-7">
        <h2> 7. Prototyping and Implementation </h2>
        <p>This section outlines the development and technical implementation of two core prototypes that contributed to the final VR dental training system. The first prototype focused on designing the patient's conversational behavior through prompt engineering and persona development using a knowledge-based chatbot. The second prototype introduced a fully interactive virtual patient within a 3D clinical environment, enabling real-time speech interaction in VR. Each prototype addressed specific aspects of the system, including dialogue flow, voice interaction, and emotional realism, and was refined through iterative testing and feedback.</p>
        <div id="sub-section-7-header-1">
          <h3>7.1 Prototype 1: Knowledge Chatbot for Prompt Testing</h3>
          <p>The first prototype was a desktop-based knowledge chatbot designed to test the conversational behavior of a virtual geriatric patient using large language models (LLMs). The objective was to simulate natural, unscripted dialogue that reflected geriatric communication challenges such as memory lapses, confusion, and emotional variability. This sandbox environment enabled rapid iteration on prompt structure and patient logic without the constraints of a VR interface.</p>
          <div id="sub-section-7-header-1-1">
            <h4>7.1.1 Prompt Engineering Process</h4>
            <p>Prompt engineering played a critical role in controlling the simulated behavior of Shibing, a 78-year-old Singaporean Chinese man with early-stage dementia. Prompts were designed to reflect speech hesitancy, informal Singlish phrasing, and fragmented memory. Each prompt contained:</p>
            <ul>
              <li>A short summary of the recent dialogue history.</li>
              <li>System-level instructions defining Shibing‚Äôs personality, behavior, and constraints.</li>
              <li>Emotional flags indicating confusion, defensiveness, or frustration in response to specific user inputs.</li>
            </ul>
            <p>The prompt design was refined over multiple versions, with improvements focusing on character consistency, fallback responses, and persona realism. Fallback expressions such as ‚ÄúHuh? What you mean lah?‚Äù or ‚ÄúAiyo, I forget liao leh‚Ä¶‚Äù were added to simulate dementia-related confusion and miscommunication.</p>        
          </div>
          <div id="sub-section-7-header-1-2">
            <h4>7.1.2 Technical Setup</h4>
            <p>The backend was implemented using Python, with the following core modules:</p>
              <ul>
                <li><code>conversation.py</code>: Managed turn-based memory, confusion triggers, and persona grounding.</li>
                <li><code>completions.py</code>: Handled GPT-4o mini interactions.</li>
                <li><code>messages.txt</code>: Contained fallback responses for miscommunication scenarios.</li>
                <li><code>websocket.py</code>: Reserved for integration with frontend interfaces during testing.</li>
              </ul>
              <p>A basic interface was used to simulate input/output, enabling structured evaluations of dialogue flow, memory integrity, and emotional variability. Insights from this stage directly informed system design choices in the second prototype.</p>
          </div>
        </div>
        <div id="sub-section-7-header-2">
          <h3>7.2 Prototype 2: Immersive VR Patient (Ahmed)</h3>
          <p>The second prototype built upon the previous chatbot logic and deployed it within an immersive VR environment. A virtual patient named Ahmed was developed to support real-time, speech-based interaction in a simulated dental consultation setting. Ahmed was similar to Shibing, the change was that he was a Singaporean Malay individual instead. This prototype tested end-to-end integration of voice capture, LLM-based response generation, avatar behavior, and emotion-driven animation within a clinical training context.</p>
          <div id="sub-section-7-header-2-1">
            <h4>7.2.1 System Architecture</h4>
            <p>The system architecture comprises a Python backend and a Unity frontend, connected via WebSocket for real-time interaction. This modular design enables scalable updates across the conversational logic and visual rendering components.</p>
          </div>
          
          <div id="sub-section-7-header-2-2">
            <h4>7.2.2 Backend (Python)</h4>
            <ul>
              <li><code>conversation.py</code>: Maintains session state, fallback conditions, and persona logic.</li>
              <li><code>completions.py</code>: Sends prompt data to GPT-4o mini and processes responses.</li>
              <li><code>websocket.py</code>: Facilitates real-time communication with Unity.</li>
              <li><code>decoder.py</code> and <code>embeddings.py</code>: Scaffolded for future emotion and memory extensions.</li>
              <li><code>.env</code> and <code>secrets.py</code>: Manage API credentials and environment variables securely.</li>
            </ul>
          </div>
          
          <div id="sub-section-7-header-2-3">
            <h4>7.2.3 Frontend (Unity)</h4>
            <ul>
              <li><code>SimpleMicStreamInstance</code>: Captures voice input from the MetaQuest microphone.</li>
              <li><code>AudioStreamInstance</code>: Plays TTS output and synchronizes mouth movement.</li>
              <li><code>TurnTakerController</code> and <code>MuteUnmuteController</code>: Manage dialogue pacing and turn-taking.</li>
              <li><code>LipSyncInput</code>: Maps TTS phonemes to blendshape animations for lip syncing.</li>
            </ul>
          </div>
          
          <div id="sub-section-7-header-2-4">
            <h4>7.2.4 Scene Design and VR Interaction</h4>
            <p>The environment was built in Unity and adapted from an existing aviation training prototype to simulate a compact dental consultation room. Key elements included:</p>
            <ul>
              <li>A dental chair, clinical cabinetry, and relevant equipment.</li>
              <li>Ambient background audio and realistic lighting.</li>
              <li>A seated interaction design using <code>OVRCameraRig</code> to match real-world student posture and reduce VR fatigue.</li>
            </ul>
            <p>The scene is modular and designed for future integration with alternative patients or scenarios. Users interact entirely through voice, enabling natural dialogue in an immersive context.</p>
          </div>
          
          <div id="sub-section-7-header-2-5">
            <h4>7.2.5 Speech API Integration and Unity Timeline</h4>
            <p>Real-time voice interaction relied on a tightly integrated STT ‚Üí LLM ‚Üí TTS loop:</p>
            <ul>
              <li>Azure STT transcribes student speech.</li>
              <li>Python backend formats input, adds memory/emotion context, and sends to GPT-4o mini.</li>
              <li>GPT-generated output is synthesized into audio via Azure TTS.</li>
              <li>Unity receives the audio stream, plays it, and triggers lip-sync and animations.</li>
            </ul>
            <p>Turn-taking logic prevents crosstalk and manages conversational flow. During scripted testing or demonstrations, the Unity Timeline system was used to choreograph camera movement, emotional expression, and dialogue pacing for repeatable interactions.</p>
            <p>This high-fidelity VR prototype confirmed the feasibility of integrating AI-driven behavior, emotional nuance, and immersive design into a scalable clinical training system.</p>
          </div>
          
     
        </div> 

      <br />
      <sl-divider></sl-divider>
      <div id="section-header-8">
        <h2>8. Testing and Evaluation</h2>
        <p>To assess the effectiveness of the AI-simulated geriatric patient, two rounds of user testing were carried out. The first involved participants from engineering and computing backgrounds to gather preliminary feedback, while the second focused on domain-specific evaluation from dental students familiar with clinical communication training.</p>
      </div>

      <div id="sub-section-8-header-1">
        <h3>8.1 Preliminary Testing with Non-Dental Students</h3>
        <p>The initial round of testing was conducted with students from engineering and computing disciplines. Participants interacted with the AI patient within the VR environment and provided qualitative feedback on their experience. Several key insights emerged:</p>
        <ul>
          <li><strong>Patient Proximity:</strong> Participants felt the AI patient appeared too distant in the VR space, which reduced the sense of interpersonal connection.</li>
          <li><strong>Conversation Flow:</strong> The dialogue was perceived as natural, with a coherent and responsive back-and-forth exchange.</li>
          <li><strong>Language Style:</strong> While the Singlish-influenced dialogue was considered strong, most users acknowledged it was appropriate given the patient‚Äôs local background. Non-local participants found it understandable and not disruptive.</li>
          <li><strong>Immersion:</strong> The immersive environment and the novelty of speaking with an elderly virtual patient were positively received.</li>
        </ul>
        <p>This round of testing helped identify early design concerns, particularly related to spatial positioning and dialect authenticity, which informed refinements ahead of the next phase.</p>
      </div>

      <div id="sub-section-8-header-2">
        <h3>8.2 Domain-Specific Testing with Dental Students</h3>
        <p>A second round of testing was conducted on April 2nd with five dental students. This round aimed to evaluate how well the AI patient replicated actor-based patient scenarios typically used in clinical education. Participants provided written feedback on several key areas:</p>
        <ul>
          <li><strong>Realism:</strong> Most students found the AI patient‚Äôs behavior and language to be fairly realistic. However, the use of an American-accented voice was noted to disrupt immersion. Suggestions included improving emotional tone and incorporating more locally accurate expressions.</li>
          <li><strong>Communication:</strong> The AI was generally easy to converse with, though occasional lapses in coherence were observed during longer interactions.</li>
          <li><strong>Comparison to Actor-Based Training:</strong> The AI simulation was recognised as a promising supplement to traditional methods due to its availability and scalability. However, its limitations in conveying nuanced non-verbal cues, such as facial expressions or shifts in tone, were noted.</li>
          <li><strong>Suggested Improvements:</strong> Participants recommended enhancing affective expression (e.g., confusion, sadness), improving voice synthesis, and integrating in-simulation feedback tools to support reflective learning.</li>
        </ul>
        <p>Overall, this pilot study supported the potential of the system as a supplementary training tool. While the core conversational experience was found to be realistic and valuable, further development is needed to enhance emotional expressiveness and the depth of sustained dialogue.</p>
      </div>
      </div>
      </div>

      <br />
      <sl-divider></sl-divider>
      <div id="section-header-9">
        <h2> 9. Fulfilment of Deliverables </h2>
        <p>At this stage, a functional prototype has been developed featuring Ahmed, a virtual elderly patient capable of engaging in spoken dialogue within a realistic VR dental clinic setting. The system integrates real-time speech interaction, GPT-4o-based conversational AI, and an immersive environment built in Unity for the MetaQuest platform. Core objectives such as natural voice-based communication, context-aware responses, and basic emotional realism have been met. Prompt engineering has enabled the simulation of mild confusion, supportive affect, and memory-related behaviors appropriate for geriatric patient interaction. Internal testing confirmed that conversational flow was coherent and latency remained within acceptable limits.</p>
        <p>Several components are still in progress. These include the implementation of a second patient archetype, Shibing, who is Singaporean Chinese. Additionally, features such as a post-session feedback and analytics dashboard, non-verbal facial expression animations, and expanded emotional behaviors are planned for future development. Informal feedback from dental students has helped validate the prototype‚Äôs value as a training tool, while highlighting areas for further refinement. While some stretch goals remain ongoing, the project has successfully achieved its core aim of creating a realistic, AI-driven geriatric patient simulation for dental education, laying a strong foundation for iterative improvements and future expansion.</p>
      </div>

      <br />
      <sl-divider></sl-divider>
      <div id="section-header-10">
        <h2> 10. Reflections and Future Work </h2>
        <p>This project presented several technical and design challenges that required adaptation and iterative problem-solving. One of the initial constraints was the need to shift development to a Windows-based system, which was better suited for Unity and Visual Studio Code integration. While this improved compatibility, it introduced additional setup and configuration work that affected development timelines. Another major hurdle involved integrating microphone input in Unity. The voice input pipeline did not function reliably for an extended period, delaying progress on real-time speech interaction. Resolving this required extensive debugging and workarounds before a stable integration with Python and the GPT-4o API could be established.</p>
        <p>Prompt engineering also proved more complex than expected. Early versions of the simulated patient failed to stay in character or responded with medically inaccurate or irrelevant information. This required multiple iterations to rewrite both system instructions and conversational constraints, as well as adjustments to the codebase to ensure the LLM could maintain context and express behavior consistent with geriatric communication patterns, particularly in the case of the dementia-simulating persona, Shibing.</p>
        <p>Looking ahead, several key features are planned for future development. One priority is the creation of an analytics and feedback dashboard that can provide students with post-session insights based on communication rubrics. This will support more structured learning and reflection. Additionally, the system will be expanded to include a wider range of patient archetypes, such as patients with emotional distress, language barriers, or more complex cognitive conditions. These additions will allow for a broader range of training scenarios and further enhance the realism and educational value of the platform.</p>
      </div>

      <br />
      <sl-divider></sl-divider>
      <div id="section-header-11">
        <h2> 11. Conclusion </h2>
        <p>This project successfully delivered a working prototype of a virtual geriatric patient designed for dental education. By integrating conversational AI with a fully immersive VR environment, the system gives students an opportunity to practise complex communication skills in a setting that mirrors real-life consultations. The current prototype supports voice interaction, emotional tone, and realistic persona behavior, all within a simulated clinical space.</p>
        <p>There are still areas to develop further, such as enhancing emotional expressiveness and integrating post-session feedback. However, the system already shows strong potential as a supplementary training tool. It provides a foundation that can be expanded with additional patient scenarios and educational features. Overall, the project offers a meaningful solution to the growing need for more accessible and effective training in geriatric dental communication.</p>
      </div>
    <!-- This is an example of how you can use the references component to create references -->
    <div id="references" class="references">
      <sl-divider></sl-divider>
      <h2>References</h2>
      <ul>
        <li>
          <i>ACTION PLAN FOR SUCCESSFUL AGEING 2023</i>. (2024, August 13). Ministry of Health; Government of Singapore.
https://www.moh.gov.sg/others/resources-and-statistics/action-plan-for-successful-ageing
        </li>
        <li>
          Borg, A., Parodis, I., & Skantze, G. (2024). Creating virtual patients using robots and large language models: A preliminary study with medical students. <i>Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</i>, 273‚Äì277. https://doi.org/10.1145/3610978.3640592
        </li>
        <li>
          Hillier, M., Williams, T. L., & Chidume, T. (2024). Standardization of standardized patient training in medical simulation. In <i>StatPearls</i>. StatPearls Publishing. http://www.ncbi.nlm.nih.gov/books/NBK560864/
        </li>
        <li>
          Su√°rez, A., Adanero, A., D√≠az-Flores Garc√≠a, V., Freire, Y., & Algar, J. (2022). Using a virtual patient via an artificial intelligence chatbot to develop dental students‚Äô diagnostic skills. <i>International Journal of Environmental Research and Public Health</i>, 19(14), 8735. https://doi.org/10.3390/ijerph19148735
        </li>
      </ul>
    </div>
  </div>

  <!-- This is the code to display the scroll to top button for ergonomic -->
  <!-- You can leave it as it is, or if you don't like its aesthetics you can also just delete it, -->
  <!-- but it might reduce the user experience. -->
  <sl-button class="scroll-to-top" variant="primary" size="medium" circle onclick="scrollToTop()">
    <sl-icon name="arrow-up" label="Settings"></sl-icon>
  </sl-button>

  <script src="https://unpkg.com/gridjs/dist/gridjs.umd.js"></script>
  <script type="module" src="./components/table-component/table-component.js"></script>
</body>

</html>
